import torch
import random
import os
import logging
import openai
import 
import warnings

# KoElectra
from classifier import KoELECTRAforSequenceClassfication
from transformers import ElectraModel, ElectraConfig, ElectraTokenizer

# warning ì¶œë ¥ ì•ˆë˜ê²Œ
logging.getLogger("transformers").setLevel(logging.ERROR)
warnings.filterwarnings("ignore", message=".*resume_download.*", category=FutureWarning)

os.environ["CUDA_DEVICE_ORDER"]="PCI_BUS_ID"
os.environ["CUDA_VISIBLE_DEVICES"]="0"

def load_wellness_answer(category_path, answer_path):
    c_f = open(category_path, 'r')
    a_f = open(answer_path, 'r')

    category_lines = c_f.readlines()
    answer_lines = a_f.readlines()

    category = {}
    answer = {}
    for line_num, line_data in enumerate(category_lines):
        data = line_data.split('    ')
        if len(data) != 2:
            print(f"Error in category file at line {line_num}: {line_data}")
        category[data[1][:-1]] = data[0]

    for line_num, line_data in enumerate(answer_lines):
        data = line_data.split('    ')
        keys = answer.keys()
        if len(data) != 2:
            print(f"Error in answer file at line {line_num}: {line_data}")
        if (data[0] in keys):
            answer[data[0]] += [data[1][:-1]]
        else:
            answer[data[0]] = [data[1][:-1]]
    return category, answer


def load_model(checkpoint_path):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_config = ElectraConfig.from_pretrained("monologg/koelectra-base-v3-discriminator")

    model = KoELECTRAforSequenceClassfication(model_config, num_labels=432, hidden_dropout_prob=0.1)
    checkpoint = torch.load(checkpoint_path, map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.to(device)
    model.eval()

    tokenizer = ElectraTokenizer.from_pretrained("monologg/koelectra-base-v3-discriminator")
    return model, tokenizer, device

def preprocess_input(tokenizer, sent, device, max_seq_len=512):
    index_of_words = tokenizer.encode(sent)
    token_type_ids = [0] * len(index_of_words)
    attention_mask = [1] * len(index_of_words)
    padding_length = max_seq_len - len(index_of_words)
    index_of_words += [0] * padding_length
    token_type_ids += [0] * padding_length
    attention_mask += [0] * padding_length

    data = {
        'input_ids': torch.tensor([index_of_words]).to(device),
        'token_type_ids': torch.tensor([token_type_ids]).to(device),
        'attention_mask': torch.tensor([attention_mask]).to(device),
        }
    return data

def get_answer(category, answer, output, input_sentence):
    softmax_logit = torch.softmax(output[0], dim=-1).squeeze()
    max_index = torch.argmax(softmax_logit).item()
    max_index_value = softmax_logit[torch.argmax(softmax_logit)].item()

    threshold = 0.35

    selected_categories = []
    for i, value in enumerate(softmax_logit):
        if value > threshold:
            if str(i) in category:
                selected_categories.append(category[str(i)])
                #print(f"Softmax ê°’ì´ threshold({threshold}) ì´ìƒì¸ ì¹´í…Œê³ ë¦¬: {category[str(i)]}")
                print(f"ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ -> [ {category[str(i)]} ]")
    if not selected_categories:
        return "ì„ íƒëœ ì¹´í…Œê³ ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì…ë ¥í•´ì£¼ì„¸ìš”", None, max_index_value, []

    all_answers = []
    for category_name in selected_categories:
        if category_name in answer:
            all_answers.extend(answer[category_name])

    if not all_answers:
        return "ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì— ëŒ€í•œ ë‹µë³€ì´ ì—†ìŠµë‹ˆë‹¤.", None, max_index_value, []
3. "ì €ëŠ” ì‹¬ë¦¬ ìƒë‹´ì„ í•´ì£¼ëŠ” AI ê¸°ë£¡ì´ì—ìš”."
    openai.api_key = "sk-proj-zOVzKRhoYruJJhEkU24DT3BlbkFJXdYQxKnCQHXFBpfxbc2q"
    # MODEL = "gpt-3.5-turbo"
    MODEL = "gpt-4-turbo"

    # ê°ì • ë¶„ë¥˜ ëª¨ë¸ì—ì„œ ì˜ˆì¸¡í•œ ì¹´í…Œê³ ë¦¬
    predicted_category = selected_categories

    # ì…ë ¥ë°›ì€ ì‚¬ìš©ì ë¬¸ì¥
    user_input = input_sentence

    # í”„ë¡¬í”„íŠ¸ ì„¤ì •
    prompts = {
        "formal": f"ì˜ˆì¸¡í•œ ì¹´í…Œê³ ë¦¬ëŠ” '{predicted_category}'ì…ë‹ˆë‹¤. ì‚¬ìš©ì ë¬¸ì¥ê³¼ ì˜ˆì¸¡í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§¤ìš° ê³µì‹ì ì¸ ë§íˆ¬(~ë‹¤ ë¡œ ëë‚˜ëŠ”)ë¡œ ì‹¬ë¦¬ ìƒë‹´ ì±—ë´‡ì— ì“¸ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ 100ì ì´í•˜ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.",
        "casual": f"ì˜ˆì¸¡í•œ ì¹´í…Œê³ ë¦¬ëŠ” '{predicted_category}'ì…ë‹ˆë‹¤. ì‚¬ìš©ì ë¬¸ì¥ê³¼ ì˜ˆì¸¡í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„ì›€ê³¼ ê²©ë ¤ê°€ ë˜ëŠ” ì¹œê·¼í•˜ê³  í¸ì•ˆí•œ ë§íˆ¬ë¡œ ë°˜ë§ ì²´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹¬ë¦¬ ìƒë‹´ ì±—ë´‡ì— ì“¸ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ 100ì ì´í•˜ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”.",
        "polite" : f"The predicted category is '{predicted_category}. Based on your sentences and predicted categories, please create answers for your psychological counseling chatbot with a friendliness, polite tone that is helpful and encouraging. Please make your answers up to 200 characters",
        "default": f"ì˜ˆì¸¡í•œ ì¹´í…Œê³ ë¦¬ëŠ” '{predicted_category}'ì…ë‹ˆë‹¤. ì‚¬ìš©ì ë¬¸ì¥ê³¼ ì˜ˆì¸¡í•œ ì¹´í…Œê³ ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë„ì›€ê³¼ ê²©ë ¤ê°€ ë˜ëŠ”
 ë¶€ë“œëŸ¬ìš´ ë¬¸ì¥ìœ¼ë¡œ ì‹¬ë¦¬ ìƒë‹´ ì±—ë´‡ì— ì“¸ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”. ë‹µë³€ì€ 100ì ì´í•˜ë¡œ ë§Œë“¤ì–´ì£¼ì„¸ìš”."
    }

    # ì„ì‹œë¡œ ëœë¤ íƒ€ì…
   # chatbot_type = random.randint(1, 3)
    chatbot_type = 3
    match chatbot_type:
        case 1:
            prompt = prompts.get(chatbot_type, prompts["formal"])    # ê³µì‹ì ì´ê³  ì˜ˆì˜ë°”ë¥¸ ë§íˆ¬
            print('<ì‚¬ë¬´ì ì¸ ë§íˆ¬>')
        case 2:
            prompt = prompts.get(chatbot_type, prompts["casual"])    # ì¹œê·¼í•˜ê³  í¸ì•ˆí•œ ë§íˆ¬ë¡œ ë°˜ë§ ì²´
            print('<ì¹œê·¼í•œ ë°˜ë§íˆ¬>')
        case 3:
            prompt = prompts.get(chatbot_type, prompts["polite"])    # ì˜ˆì˜ë°”ë¥´ë©´ì„œë„ ë¶€ë“œëŸ½ê³  ì˜¨í™”í•œ ë§íˆ¬
            print('<ë¶€ë“œëŸ¬ìš´ ë§íˆ¬>')
        case _:
            prompt = prompts.get(chatbot_type, prompts["default"])   # ê¸°ë³¸


    messages = [
        {"role": "system", "content": prompt},
        {"role": "user", "content": user_input}
    ]

    response = openai.ChatCompletion.create(
        model=MODEL,
        messages=messages,
        temperature=0.9,
        max_tokens=200,
        n=1,
    )

    # ì‘ë‹µ ì²˜ë¦¬
    original_response = response.choices[0].message.content
    if not original_response.endswith('.'):
        last_sentence = re.split(r'[.!?]', original_response)[-1].strip()
        if last_sentence:
            final_response = original_response + '.'
        else:
            final_response = original_response
    else:
        final_response = original_response

    return final_response


def chat(message):
    root_path = "."
    answer_path = f"{root_path}/data/new_answer.txt"
    category_path = f"{root_path}/data/new_category_v2.txt"
    checkpoint_path = f"{root_path}/checkpoint/new_electra_v5.pth"

    category, answer = load_wellness_answer(category_path, answer_path)
    model, tokenizer, device = load_model(checkpoint_path)

    sent = str(message)

    data = preprocess_input(tokenizer, sent, device, 512)
    output = model(**data)
    answer, category, max_index_value, all_answers = get_answer(category, answer, output, sent)

    chatbot_answer = gpt(sent,category)
    print(f"\nğŸ¤– ì±—ë´‡ : {chatbot_answer}")
    print()

    return chatbot_answer